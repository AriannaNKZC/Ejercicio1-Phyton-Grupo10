{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9b23c6-3069-47c8-bdd7-b5e99a6e04a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rossy\\anaconda3\\envs\\selenium_env_2\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw\n",
    "!pip install python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b1938b-e0f1-4295-bc95-3d2a70dc56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescatamos los datos secretos desde el archivo \"ID..env\"\n",
    "load_dotenv(\"ID.env\")\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e77ead6-01b4-4c2a-9d15-f9cc57ea0f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Sunday Daily Thread: What's everyone working on this week?\n",
      "Score: 6\n",
      "URL: https://www.reddit.com/r/Python/comments/1nmdhrp/sunday_daily_thread_whats_everyone_working_on/\n",
      "\n",
      "Título: Tuesday Daily Thread: Advanced questions\n",
      "Score: 12\n",
      "URL: https://www.reddit.com/r/Python/comments/1no2n2e/tuesday_daily_thread_advanced_questions/\n",
      "\n",
      "Título: We just launched Leapcell, deploy 20 Python websites for free\n",
      "Score: 61\n",
      "URL: https://www.reddit.com/r/Python/comments/1nnh6g0/we_just_launched_leapcell_deploy_20_python/\n",
      "\n",
      "Título: D&D Twitch bot: Update 2!\n",
      "Score: 7\n",
      "URL: https://www.reddit.com/r/Python/comments/1nnwn2h/dd_twitch_bot_update_2/\n",
      "\n",
      "Título: Append-only time-series storage in pure Python: Chronostore (faster than CSV & Parquet)\n",
      "Score: 16\n",
      "URL: https://www.reddit.com/r/Python/comments/1nnn35x/appendonly_timeseries_storage_in_pure_python/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probamos conexión con Python\n",
    "for submission in reddit.subreddit(\"python\").hot(limit=5):\n",
    "    print(f\"Título: {submission.title}\")\n",
    "    print(f\"Score: {submission.score}\")\n",
    "    print(f\"URL: {submission.url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9479b-6859-4866-848e-b07f7c0a9024",
   "metadata": {},
   "source": [
    "## PARTE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a96d5a-658e-49b4-a646-b5aa989b1213",
   "metadata": {},
   "source": [
    "Target subreddits:\n",
    "- r/politics\n",
    "- r/PoliticalDiscussion\n",
    "- r/worldnews\n",
    "Task: For each of the three subreddits, collect 20 “hot” or “top” posts per subreddit.\n",
    "Extraction (for each post, extract):\n",
    "- title\n",
    "- score (upvotes)\n",
    "- num_comments\n",
    "- id (unique identifier)\n",
    "- url\n",
    "Storage: Store this post data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3154b48c-dc99-4a57-8d6b-3f80b11feb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recolectando posts de: r/politics ...\n",
      "Recolectando posts de: r/PoliticalDiscussion ...\n",
      "Recolectando posts de: r/worldnews ...\n",
      " Extracción completada. Total de posts recolectados: 60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Indicamos los subreddits\n",
    "subreddits = [\"politics\", \"PoliticalDiscussion\", \"worldnews\"]\n",
    "# Inicializamos la lista de posts\n",
    "all_posts = []\n",
    "\n",
    "# Recorremos cada subreddit\n",
    "for sub in subreddits:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    print(f\"Recolectando posts de: r/{sub} ...\")\n",
    "\n",
    "    for submission in subreddit.hot(limit=20):  # puedes cambiar hot → top\n",
    "        all_posts.append({\n",
    "            \"subreddit\": sub,\n",
    "            \"title\": submission.title,\n",
    "            \"score\": submission.score,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"id\": submission.id,\n",
    "            \"url\": submission.url\n",
    "        })\n",
    "\n",
    "print(f\" Extracción completada. Total de posts recolectados: {len(all_posts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb9185d-0954-45ae-b445-fc02e57d888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de posts por subreddit:\n",
      "subreddit\n",
      "politics               20\n",
      "PoliticalDiscussion    20\n",
      "worldnews              20\n",
      "Name: count, dtype: int64\n",
      "✅ Archivo CSV creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Convertimos a DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Verificación rápida\n",
    "print(\"Total de posts por subreddit:\")\n",
    "print(df[\"subreddit\"].value_counts())\n",
    "\n",
    "# Guardamos a CSV\n",
    "df.to_csv(\"reddit_posts_politics_world.csv\", index=False)\n",
    "\n",
    "print(\"✅ Archivo CSV creado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef55945-e0ff-4bc1-b752-5db0268af796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
