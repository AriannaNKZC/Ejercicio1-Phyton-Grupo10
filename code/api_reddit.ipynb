{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bbdf42-d8b7-4e68-8356-8054d1aa75a3",
   "metadata": {},
   "source": [
    "## Part 2: Collect Data and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8202e58f-2e63-4b83-9f32-fdc1df838b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Obtaining dependency information for praw from https://files.pythonhosted.org/packages/73/ca/60ec131c3b43bff58261167045778b2509b83922ce8f935ac89d871bd3ea/praw-7.8.1-py3-none-any.whl.metadata\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Obtaining dependency information for prawcore<3,>=2.4 from https://files.pythonhosted.org/packages/96/5c/8af904314e42d5401afcfaff69940dc448e974f80f7aa39b241a4fbf0cf1/prawcore-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Obtaining dependency information for update_checker>=0.18 from https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\rossy\\anaconda3\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\rossy\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\rossy\\anaconda3\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rossy\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rossy\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rossy\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rossy\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.3 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 92.2/189.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 189.3/189.3 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rossy\\anaconda3\\lib\\site-packages (0.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw\n",
    "!pip install python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740c49ec-a516-4d15-bc30-4c25d68b0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescatamos los datos secretos desde el archivo \"ID..env\"\n",
    "load_dotenv(\"ID.env\")\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ac4f38-b12e-4ea2-9746-65694254dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Sunday Daily Thread: What's everyone working on this week?\n",
      "Score: 6\n",
      "URL: https://www.reddit.com/r/Python/comments/1nmdhrp/sunday_daily_thread_whats_everyone_working_on/\n",
      "\n",
      "Título: Thursday Daily Thread: Python Careers, Courses, and Furthering Education!\n",
      "Score: 2\n",
      "URL: https://www.reddit.com/r/Python/comments/1nps3nn/thursday_daily_thread_python_careers_courses_and/\n",
      "\n",
      "Título: Pyrefly & Instagram - A Case Study on the Pain of Slow Code Navigation\n",
      "Score: 89\n",
      "URL: https://www.reddit.com/r/Python/comments/1np9d42/pyrefly_instagram_a_case_study_on_the_pain_of/\n",
      "\n",
      "Título: Teaching my wife python!\n",
      "Score: 16\n",
      "URL: https://www.reddit.com/r/Python/comments/1nplhop/teaching_my_wife_python/\n",
      "\n",
      "Título: Fast API better option than Django?\n",
      "Score: 35\n",
      "URL: https://www.reddit.com/r/Python/comments/1npercr/fast_api_better_option_than_django/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probamos conexión con Python\n",
    "for submission in reddit.subreddit(\"python\").hot(limit=5):\n",
    "    print(f\"Título: {submission.title}\")\n",
    "    print(f\"Score: {submission.score}\")\n",
    "    print(f\"URL: {submission.url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e00c24-8455-4aa0-bb3f-9a7ca2e28a3f",
   "metadata": {},
   "source": [
    "### PARTE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092da1d6-299e-4bbf-8986-4546ded6de61",
   "metadata": {},
   "source": [
    "Target subreddits:\n",
    "\n",
    "- r/politics\n",
    "- r/PoliticalDiscussion\n",
    "- r/worldnews\n",
    "Task: For each of the three subreddits, collect 20 “hot” or “top” posts per subreddit. Extraction (for each post, extract):\n",
    "- title\n",
    "- score (upvotes)\n",
    "- num_comments\n",
    "- id (unique identifier)\n",
    "- url Storage: Store this post data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c0082a-5b15-4185-b3c1-5fa9bb9dab1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recolectando posts de: r/politics ...\n",
      "Recolectando posts de: r/PoliticalDiscussion ...\n",
      "Recolectando posts de: r/worldnews ...\n",
      " Extracción completada. Total de posts recolectados: 60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Indicamos los subreddits\n",
    "subreddits = [\"politics\", \"PoliticalDiscussion\", \"worldnews\"]\n",
    "# Inicializamos la lista de posts\n",
    "all_posts = []\n",
    "\n",
    "# Recorremos cada subreddit\n",
    "for sub in subreddits:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    print(f\"Recolectando posts de: r/{sub} ...\")\n",
    "\n",
    "    for submission in subreddit.hot(limit=20):  # puedes cambiar hot → top\n",
    "        all_posts.append({\n",
    "            \"subreddit\": sub,\n",
    "            \"title\": submission.title,\n",
    "            \"score\": submission.score,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"id\": submission.id,\n",
    "            \"url\": submission.url\n",
    "        })\n",
    "\n",
    "print(f\" Extracción completada. Total de posts recolectados: {len(all_posts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9bdefe-3250-427a-abde-7eb8e6a168ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de posts por subreddit:\n",
      "subreddit\n",
      "politics               20\n",
      "PoliticalDiscussion    20\n",
      "worldnews              20\n",
      "Name: count, dtype: int64\n",
      "Archivo CSV creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Convertimos a DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Verificación rápida\n",
    "print(\"Total de posts por subreddit:\")\n",
    "print(df[\"subreddit\"].value_counts())\n",
    "\n",
    "# Guardamos a CSV\n",
    "df.to_csv(\"reddit_posts_politics_world.csv\", index=False)\n",
    "\n",
    "print(\"Archivo CSV creado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041c7db",
   "metadata": {},
   "source": [
    "Ahora, para subconjuntos de publicaciones relevantes, recopilaremos 5 comentarios por publicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d45988-01c3-4a66-a145-98704e9ef262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3298cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = df.sort_values(\"num_comments\", ascending=False).head(10)\n",
    "all_comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7390e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in top_posts.iterrows(): \n",
    "    post_id = row[\"id\"]\n",
    "    try:\n",
    "        submission = reddit.submission(id=post_id)\n",
    "        submission.comments.replace_more(limit=0)  # expandir comentarios\n",
    "\n",
    "        # Ordenamos y tomamos los 5 mejores comentarios\n",
    "        comments_sorted = sorted(\n",
    "            submission.comments.list(),\n",
    "            key=lambda c: getattr(c, \"score\", 0),\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        for c in comments_sorted:\n",
    "            all_comments.append({\n",
    "                \"post_id\": post_id,             \n",
    "                \"comment_id\": c.id,            \n",
    "                \"body\": c.body,                 \n",
    "                \"score\": c.score,               \n",
    "                \"created_utc\": c.created_utc    \n",
    "            })\n",
    "\n",
    "        time.sleep(0.5)  #para evitar rate-limit\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en post {post_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff30c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentarios recolectados: 50\n",
      "Archivo 'reddit_comments_politics_world.csv' creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Convertimos a DataFrame\n",
    "df_comments = pd.DataFrame(all_comments)\n",
    "\n",
    "# Guardamos a CSV\n",
    "df_comments.to_csv(\"reddit_comments_politics_world.csv\", index=False)\n",
    "\n",
    "print(f\"Comentarios recolectados: {len(df_comments)}\")\n",
    "print(\"Archivo 'reddit_comments_politics_world.csv' creado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2f304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
